{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# Object Detection using Tensorflow Object Detection API\n",
    "\n",
    "In this lab exercise, we will use our trained custom object detection model to detect balloons in both images and videos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPs64QA1Zdov"
   },
   "source": [
    "## Imports and Setup\n",
    "\n",
    "Let's start with the base imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yn5_uV1HLvaz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IogyryF2lFBL"
   },
   "source": [
    "## Utilities\n",
    "\n",
    "Run the following cell to create some utils that will be needed later:\n",
    "\n",
    "- Helper method to load an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-y9R0Xllefec"
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "    path: the file path to the image\n",
    "\n",
    "    Returns:\n",
    "    uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    image = None\n",
    "    if(path.startswith('http')):\n",
    "        response = urlopen(path)\n",
    "        image_data = response.read()\n",
    "        image_data = BytesIO(image_data)\n",
    "        image = Image.open(image_data)\n",
    "    else:\n",
    "        image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "        image = Image.open(BytesIO(image_data))\n",
    "\n",
    "        (im_width, im_height) = image.size\n",
    "        \n",
    "    return np.array(image.getdata())[:,:3].reshape(\n",
    "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14bNk1gzh0TN"
   },
   "source": [
    "## Visualization tools\n",
    "\n",
    "To visualize the images with the proper detected boxes, we will use the TensorFlow Object Detection API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JCeQU3fkayh"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKtD0IeclbL5"
   },
   "source": [
    "### Load label map data (for plotting).\n",
    "\n",
    "Label maps correspond index numbers to category name. For example, if our COCO-trained model predicts `5`, we know that this corresponds to `airplane`. Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine.\n",
    "\n",
    "We will load the label_map we created for our balloon detector, which only contains one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mucYUS6exUJ"
   },
   "outputs": [],
   "source": [
    "PATH_TO_LABELS = 'data/label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muhUt-wWL582"
   },
   "source": [
    "## Loading our saved custom model\n",
    "\n",
    "Here we will load our trained balloon detection model and use it later to detect our picture and videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following if you want to use a pretrained balloon detection model\n",
    "\n",
    "# !wget https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/pretrained-models/balloon_model.tar.gz\n",
    "# !mkdir mymodel\n",
    "# !tar xzvf balloon_model.tar.gz -C mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBuD07fLlcEO"
   },
   "outputs": [],
   "source": [
    "model = 'ssd_mobilenet_v2_320x320_coco17_tpu-8'\n",
    "experiment='run1'\n",
    "export_root_dir = '/home/ubuntu/balloon_project/exported_models'\n",
    "\n",
    "# Uncomment the following if you want to use a pretrained balloon detection model\n",
    "# model_dir = 'mymodel/saved_model'\n",
    "model_dir = os.path.join(export_root_dir, f'{model}/{experiment}/saved_model')\n",
    "\n",
    "# load the model\n",
    "detection_model = tf.saved_model.load(str(model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIawRDKPPnd4"
   },
   "source": [
    "## Loading an image\n",
    "\n",
    "Let's try the model on your own image. You can upload your image using the code below. The code below uses ipywidgets [FileUpload](https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20List.html) widget to upload file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='image/*',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
    "    multiple=False  # True to accept multiple files upload else False\n",
    ")\n",
    "\n",
    "display(uploader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After upload, we need to write the upload data to a file. The content of the upload data can be found in ``uploader.data[0]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = next(iter(uploader.value))\n",
    "\n",
    "samples_folder = 'test_samples'\n",
    "\n",
    "if not os.path.exists(samples_folder):\n",
    "    os.mkdir(samples_folder)\n",
    "\n",
    "image_path = os.path.join(samples_folder, key)\n",
    "\n",
    "with open(image_path, \"w+b\") as file:\n",
    "    file.write(uploader.data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hX-AWUQ1wIEr"
   },
   "outputs": [],
   "source": [
    "image_np = load_image_into_numpy_array(image_path)\n",
    "plt.figure(figsize=(24,32))\n",
    "plt.imshow(image_np[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTHsFjR6HNwb"
   },
   "source": [
    "## Doing the inference\n",
    "\n",
    "To do the inference we just need to call our detection model. The detection model expect the following input and output a dictionary of items described below:\n",
    "\n",
    "*Inputs*:\n",
    "\n",
    "A three-channel image of variable size - the model does NOT support batching. The input tensor is a tf.uint8 tensor with shape [1, height, width, 3] with values in [0, 255].\n",
    "\n",
    "*Outputs*:\n",
    "\n",
    "The output dictionary contains:\n",
    "\n",
    "- num_detections: a tf.int tensor with only one value, the number of detections [N].\n",
    "\n",
    "- detection_boxes: a tf.float32 tensor of shape [N, 4] containing bounding box coordinates in the following order: [ymin, xmin, ymax, xmax].\n",
    "- detection_classes: a tf.int tensor of shape [N] containing detection class index from the label file.\n",
    "- detection_scores: a tf.float32 tensor of shape [N] containing detection scores.\n",
    "- raw_detection_boxes: a tf.float32 tensor of shape [1, M, 4] containing decoded detection boxes without Non-Max suppression. M is the number of raw detections.\n",
    "- raw_detection_scores: a tf.float32 tensor of shape [1, M, 90] and contains class score logits for raw detection boxes. M is the number of raw detections.\n",
    "- detection_anchor_indices: a tf.float32 tensor of shape [N] and contains the anchor indices of the detections after NMS.\n",
    "- detection_multiclass_scores: a tf.float32 tensor of shape [1, N, 90] and contains class score distribution (including background) for detection boxes in the image including background class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gb_siXKcnnGC"
   },
   "outputs": [],
   "source": [
    "# running inference\n",
    "results = detection_model(image_np)\n",
    "\n",
    "# different object detection models have additional results\n",
    "result = {key:value.numpy() for key,value in results.items()}\n",
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ5VYaBoeeFM"
   },
   "source": [
    "## Visualizing the results\n",
    "\n",
    "Here is where we will need the TensorFlow Object Detection API to show the squares from the inference step (and the keypoints when available). The full documentation of this method can be seen [here](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py)\n",
    "\n",
    "Here you can, for example, set `min_score_thresh` to other values (between 0 and 1) to allow more detections in or to filter out more detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2O7rV8g9s8Bz"
   },
   "outputs": [],
   "source": [
    "label_id_offset = 0\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np_with_detections[0],\n",
    "      result['detection_boxes'][0],\n",
    "      (result['detection_classes'][0] + label_id_offset).astype(int),\n",
    "      result['detection_scores'][0],\n",
    "      category_index,\n",
    "      use_normalized_coordinates=True,\n",
    "      max_boxes_to_draw=200,\n",
    "      min_score_thresh=.60,\n",
    "      agnostic_mode=False,\n",
    "      line_thickness=2)\n",
    "\n",
    "plt.figure(figsize=(24,32))\n",
    "plt.imshow(image_np_with_detections[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection on Video\n",
    "\n",
    "We can also do object detection on a video file. We just treat our video file as a sequence of frames (images) and we run the detector on each frame. After we draw the bounding boxes on the frame (image), we then write the new image (with bounding boxes) to a video file, frame by frame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_video(video_in_filepath, video_out_filepath, detection_model):\n",
    "    if not os.path.exists(video_in_filepath):\n",
    "        print('video filepath not valid')\n",
    "    \n",
    "    video_reader = cv2.VideoCapture(video_in_filepath)\n",
    "    \n",
    "    nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = video_reader.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    video_writer = cv2.VideoWriter(video_out_filepath,\n",
    "                               cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "                               fps, \n",
    "                               (frame_w, frame_h))\n",
    "\n",
    "    for i in tqdm(range(nb_frames)):\n",
    "        ret, image_np = video_reader.read()\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.uint8)\n",
    "        results = detection_model(input_tensor)\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                  image_np,\n",
    "                  results['detection_boxes'][0].numpy(),\n",
    "                  (results['detection_classes'][0].numpy()+ label_id_offset).astype(int),\n",
    "                  results['detection_scores'][0].numpy(),\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True,\n",
    "                  max_boxes_to_draw=200,\n",
    "                  min_score_thresh=.50,\n",
    "                  agnostic_mode=False,\n",
    "                  line_thickness=2)\n",
    "\n",
    "        video_writer.write(np.uint8(image_np))\n",
    "                \n",
    "    # Release camera and close windows\n",
    "    video_reader.release()\n",
    "    video_writer.release() \n",
    "    cv2.destroyAllWindows() \n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploader = widgets.FileUpload(\n",
    "    accept='.mp4',  # Accepted file extension only '.mp4'\n",
    "    multiple=False  # True to accept multiple files upload else False\n",
    ")\n",
    "\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = next(iter(uploader.value))\n",
    "\n",
    "samples_folder = 'test_samples'\n",
    "\n",
    "if not os.path.exists(samples_folder):\n",
    "    os.mkdir(samples_folder)\n",
    "video_in_file = os.path.join(samples_folder, key)\n",
    "with open(video_in_file, \"w+b\") as file:\n",
    "    file.write(uploader.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "basename = Path(video_in_file).stem\n",
    "video_out_file = os.path.join(samples_folder, basename + '_detected' + '.mp4')\n",
    "\n",
    "write_video(video_in_file, video_out_file, detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Object Detection Inference on TF 2 and TF Hub",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
